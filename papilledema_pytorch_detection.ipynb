{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8165aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "996ca5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3297bbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available: 20\n"
     ]
    }
   ],
   "source": [
    "cpu_cores = os.cpu_count()\n",
    "print(\"Number of CPU cores available:\", cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcbf6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/LOQ/Desktop/Grad Project/pap_backup/papilledema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18fddc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ece3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "714b0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
    "\n",
    "n_total = len(full_dataset)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val   = int(0.15 * n_total)\n",
    "n_test  = n_total - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_ds.dataset.transform = train_transform\n",
    "val_ds.dataset.transform = base_transform\n",
    "test_ds.dataset.transform = base_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f0211c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "768ef60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class allPap_Grade(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 × 224 × 224 input\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 112 × 112\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 56 × 56\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 28 × 28\n",
    "\n",
    "            # Block 4 (more abstract features)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   \n",
    "               \n",
    "            # Block 4 (more abstract features)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 14 × 14\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),      # 256 × 1 × 1\n",
    "            nn.Flatten(),                  # 256\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),               # ← Add after first ReLU\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),               # ← Add after second ReLU\n",
    "            nn.Linear(128, 6)              # Keep final layer without dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f6e7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = allPap_Grade().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7952ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)  # [B] class indices 0..5\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(xb)           # [B, 6]\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d516b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.7076 acc=0.337 | val_loss=1.7885 acc=0.208\n",
      "Epoch 02 | train_loss=1.4877 acc=0.635 | val_loss=1.6850 acc=0.509\n",
      "Epoch 03 | train_loss=1.2867 acc=0.703 | val_loss=1.5905 acc=0.509\n",
      "Epoch 04 | train_loss=1.1048 acc=0.727 | val_loss=1.4791 acc=0.491\n",
      "Epoch 05 | train_loss=0.9905 acc=0.755 | val_loss=1.3268 acc=0.547\n",
      "Epoch 06 | train_loss=0.8564 acc=0.763 | val_loss=1.1110 acc=0.623\n",
      "Epoch 07 | train_loss=0.7799 acc=0.759 | val_loss=0.9795 acc=0.717\n",
      "Epoch 08 | train_loss=0.7356 acc=0.779 | val_loss=0.9969 acc=0.679\n",
      "Epoch 09 | train_loss=0.7289 acc=0.759 | val_loss=1.0856 acc=0.623\n",
      "Epoch 10 | train_loss=0.6878 acc=0.779 | val_loss=0.9371 acc=0.698\n",
      "Epoch 11 | train_loss=0.6376 acc=0.799 | val_loss=1.0556 acc=0.566\n",
      "Epoch 12 | train_loss=0.6326 acc=0.775 | val_loss=0.9841 acc=0.698\n",
      "Epoch 13 | train_loss=0.6111 acc=0.783 | val_loss=1.2703 acc=0.585\n",
      "Epoch 14 | train_loss=0.6345 acc=0.771 | val_loss=1.0491 acc=0.660\n",
      "Epoch 15 | train_loss=0.5297 acc=0.819 | val_loss=0.9239 acc=0.717\n",
      "Epoch 16 | train_loss=0.5445 acc=0.811 | val_loss=0.9757 acc=0.660\n",
      "Epoch 17 | train_loss=0.5394 acc=0.799 | val_loss=1.1940 acc=0.509\n",
      "Epoch 18 | train_loss=0.5177 acc=0.819 | val_loss=1.0580 acc=0.642\n",
      "Epoch 19 | train_loss=0.4694 acc=0.827 | val_loss=1.1262 acc=0.623\n",
      "Epoch 20 | train_loss=0.4309 acc=0.847 | val_loss=1.0335 acc=0.717\n",
      "Epoch 21 | train_loss=0.4061 acc=0.867 | val_loss=0.9905 acc=0.642\n",
      "Epoch 22 | train_loss=0.4286 acc=0.839 | val_loss=0.9192 acc=0.698\n",
      "Epoch 23 | train_loss=0.4511 acc=0.831 | val_loss=1.2648 acc=0.604\n",
      "Epoch 24 | train_loss=0.4463 acc=0.859 | val_loss=1.4866 acc=0.717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      3\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m run_epoch(train_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m     val_loss,   val_acc   \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 9\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train)\u001b[0m\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m total_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     10\u001b[0m     xb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     yb \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [B] class indices 0..5\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1443\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "          f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300abc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",       # because you monitor validation loss\n",
    "    factor=0.1,       # reduce LR by ×0.1\n",
    "    patience=3        # epochs with no improvement before reducing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b4abcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m(train_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m     val_loss,   val_acc   \u001b[38;5;241m=\u001b[39m run_epoch(val_loader,   train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(60):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # step the scheduler with the validation loss\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e13a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class allPap_Grade2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 × 224 × 224 input\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 112 × 112\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 56 × 56\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 28 × 28\n",
    "\n",
    "            # Block 4 (more abstract features)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),      # 14 × 14\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),   # 256 × 1 × 1\n",
    "            nn.Flatten(),              # 256\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 6)          \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee36664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = allPap_Grade2().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8029beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.4809 acc=0.530 | val_loss=1.4868 acc=0.585\n",
      "Epoch 02 | train_loss=1.1970 acc=0.659 | val_loss=1.0035 acc=0.698\n",
      "Epoch 03 | train_loss=1.1246 acc=0.655 | val_loss=1.0738 acc=0.660\n",
      "Epoch 04 | train_loss=1.0695 acc=0.663 | val_loss=0.9756 acc=0.698\n",
      "Epoch 05 | train_loss=0.9656 acc=0.695 | val_loss=1.0793 acc=0.623\n",
      "Epoch 06 | train_loss=0.9145 acc=0.703 | val_loss=0.9757 acc=0.660\n",
      "Epoch 07 | train_loss=0.9166 acc=0.707 | val_loss=1.3212 acc=0.585\n",
      "Epoch 08 | train_loss=0.8875 acc=0.723 | val_loss=0.8444 acc=0.698\n",
      "Epoch 09 | train_loss=0.8704 acc=0.715 | val_loss=0.9770 acc=0.642\n",
      "Epoch 10 | train_loss=0.8136 acc=0.735 | val_loss=0.7791 acc=0.755\n",
      "Epoch 11 | train_loss=0.7819 acc=0.739 | val_loss=0.9430 acc=0.642\n",
      "Epoch 12 | train_loss=0.8249 acc=0.731 | val_loss=0.8392 acc=0.717\n",
      "Epoch 13 | train_loss=0.7771 acc=0.735 | val_loss=0.7592 acc=0.755\n",
      "Epoch 14 | train_loss=0.7962 acc=0.723 | val_loss=0.7910 acc=0.736\n",
      "Epoch 15 | train_loss=0.7640 acc=0.739 | val_loss=1.2361 acc=0.547\n",
      "Epoch 16 | train_loss=0.7519 acc=0.747 | val_loss=0.8357 acc=0.679\n",
      "Epoch 17 | train_loss=0.7332 acc=0.743 | val_loss=0.9126 acc=0.660\n",
      "Epoch 18 | train_loss=0.6982 acc=0.751 | val_loss=1.2482 acc=0.566\n",
      "Epoch 19 | train_loss=0.7032 acc=0.751 | val_loss=0.8166 acc=0.717\n",
      "Epoch 20 | train_loss=0.7943 acc=0.743 | val_loss=0.9466 acc=0.623\n",
      "Epoch 21 | train_loss=0.7157 acc=0.755 | val_loss=0.8036 acc=0.698\n",
      "Epoch 22 | train_loss=0.6737 acc=0.755 | val_loss=1.2197 acc=0.566\n",
      "Epoch 23 | train_loss=0.6732 acc=0.747 | val_loss=1.1361 acc=0.566\n",
      "Epoch 24 | train_loss=0.7590 acc=0.719 | val_loss=0.7655 acc=0.736\n",
      "Epoch 25 | train_loss=0.7173 acc=0.755 | val_loss=0.7706 acc=0.717\n",
      "Epoch 26 | train_loss=0.6510 acc=0.771 | val_loss=1.0947 acc=0.604\n",
      "Epoch 27 | train_loss=0.6205 acc=0.759 | val_loss=0.7800 acc=0.717\n",
      "Epoch 28 | train_loss=0.6414 acc=0.759 | val_loss=0.7366 acc=0.717\n",
      "Epoch 29 | train_loss=0.6641 acc=0.751 | val_loss=0.8666 acc=0.698\n",
      "Epoch 30 | train_loss=0.6220 acc=0.767 | val_loss=0.7296 acc=0.736\n",
      "Epoch 31 | train_loss=0.6367 acc=0.771 | val_loss=0.7965 acc=0.717\n",
      "Epoch 32 | train_loss=0.6392 acc=0.759 | val_loss=0.7083 acc=0.736\n",
      "Epoch 33 | train_loss=0.6005 acc=0.759 | val_loss=0.8617 acc=0.698\n",
      "Epoch 34 | train_loss=0.6075 acc=0.759 | val_loss=1.0016 acc=0.604\n",
      "Epoch 35 | train_loss=0.5764 acc=0.767 | val_loss=1.0035 acc=0.660\n",
      "Epoch 36 | train_loss=0.6466 acc=0.751 | val_loss=0.8644 acc=0.698\n",
      "Epoch 37 | train_loss=0.6087 acc=0.763 | val_loss=0.8502 acc=0.698\n",
      "Epoch 38 | train_loss=0.6004 acc=0.767 | val_loss=0.8431 acc=0.679\n",
      "Epoch 39 | train_loss=0.5709 acc=0.767 | val_loss=0.7197 acc=0.755\n",
      "Epoch 40 | train_loss=0.5501 acc=0.763 | val_loss=0.8169 acc=0.717\n",
      "Epoch 41 | train_loss=0.6598 acc=0.775 | val_loss=0.7437 acc=0.717\n",
      "Epoch 42 | train_loss=0.5743 acc=0.779 | val_loss=0.7340 acc=0.736\n",
      "Epoch 43 | train_loss=0.5664 acc=0.775 | val_loss=0.8995 acc=0.679\n",
      "Epoch 44 | train_loss=0.5816 acc=0.767 | val_loss=0.9685 acc=0.623\n",
      "Epoch 45 | train_loss=0.5952 acc=0.775 | val_loss=1.9321 acc=0.434\n",
      "Epoch 46 | train_loss=0.5477 acc=0.787 | val_loss=1.8467 acc=0.472\n",
      "Epoch 47 | train_loss=0.5323 acc=0.779 | val_loss=0.7769 acc=0.717\n",
      "Epoch 48 | train_loss=0.5449 acc=0.767 | val_loss=0.7655 acc=0.698\n",
      "Epoch 49 | train_loss=0.5395 acc=0.791 | val_loss=0.7594 acc=0.679\n",
      "Epoch 50 | train_loss=0.5644 acc=0.775 | val_loss=0.7159 acc=0.736\n",
      "Epoch 51 | train_loss=0.4922 acc=0.807 | val_loss=0.7843 acc=0.660\n",
      "Epoch 52 | train_loss=0.5575 acc=0.803 | val_loss=0.7253 acc=0.755\n",
      "Epoch 53 | train_loss=0.4999 acc=0.787 | val_loss=1.3625 acc=0.528\n",
      "Epoch 54 | train_loss=0.6663 acc=0.751 | val_loss=1.6284 acc=0.453\n",
      "Epoch 55 | train_loss=0.5618 acc=0.795 | val_loss=0.7457 acc=0.717\n",
      "Epoch 56 | train_loss=0.5488 acc=0.787 | val_loss=0.7600 acc=0.679\n",
      "Epoch 57 | train_loss=0.6389 acc=0.759 | val_loss=0.6965 acc=0.698\n",
      "Epoch 58 | train_loss=0.5595 acc=0.771 | val_loss=0.7485 acc=0.642\n",
      "Epoch 59 | train_loss=0.4997 acc=0.771 | val_loss=0.7890 acc=0.755\n",
      "Epoch 60 | train_loss=0.5373 acc=0.811 | val_loss=0.8909 acc=0.604\n",
      "Epoch 61 | train_loss=0.5133 acc=0.803 | val_loss=1.4775 acc=0.509\n",
      "Epoch 62 | train_loss=0.5109 acc=0.811 | val_loss=0.7799 acc=0.736\n",
      "Epoch 63 | train_loss=0.4734 acc=0.803 | val_loss=0.7843 acc=0.774\n",
      "Epoch 64 | train_loss=0.4377 acc=0.819 | val_loss=0.7970 acc=0.679\n",
      "Epoch 65 | train_loss=0.5019 acc=0.815 | val_loss=1.0513 acc=0.642\n",
      "Epoch 66 | train_loss=0.4939 acc=0.799 | val_loss=0.8351 acc=0.642\n",
      "Epoch 67 | train_loss=0.5205 acc=0.807 | val_loss=1.0475 acc=0.660\n",
      "Epoch 68 | train_loss=0.4786 acc=0.787 | val_loss=0.7581 acc=0.717\n",
      "Epoch 69 | train_loss=0.4282 acc=0.831 | val_loss=0.8614 acc=0.679\n",
      "Epoch 70 | train_loss=0.4327 acc=0.847 | val_loss=0.7752 acc=0.698\n",
      "Epoch 71 | train_loss=0.4439 acc=0.835 | val_loss=0.9220 acc=0.698\n",
      "Epoch 72 | train_loss=0.4520 acc=0.819 | val_loss=1.0583 acc=0.736\n",
      "Epoch 73 | train_loss=0.4415 acc=0.823 | val_loss=0.8169 acc=0.698\n",
      "Epoch 74 | train_loss=0.4382 acc=0.827 | val_loss=0.8845 acc=0.604\n",
      "Epoch 75 | train_loss=0.4319 acc=0.827 | val_loss=0.7245 acc=0.717\n",
      "Epoch 76 | train_loss=0.4666 acc=0.831 | val_loss=0.8787 acc=0.660\n",
      "Epoch 77 | train_loss=0.4728 acc=0.827 | val_loss=0.7897 acc=0.698\n",
      "Epoch 78 | train_loss=0.3958 acc=0.843 | val_loss=0.8524 acc=0.642\n",
      "Epoch 79 | train_loss=0.3679 acc=0.867 | val_loss=0.8085 acc=0.623\n",
      "Epoch 80 | train_loss=0.4203 acc=0.831 | val_loss=1.2566 acc=0.547\n",
      "Epoch 81 | train_loss=0.4486 acc=0.839 | val_loss=0.8473 acc=0.717\n",
      "Epoch 82 | train_loss=0.4153 acc=0.823 | val_loss=0.7859 acc=0.698\n",
      "Epoch 83 | train_loss=0.3964 acc=0.855 | val_loss=1.1721 acc=0.717\n",
      "Epoch 84 | train_loss=0.4364 acc=0.843 | val_loss=0.8134 acc=0.660\n",
      "Epoch 85 | train_loss=0.4630 acc=0.847 | val_loss=0.8467 acc=0.679\n",
      "Epoch 86 | train_loss=0.4791 acc=0.815 | val_loss=0.9607 acc=0.660\n",
      "Epoch 87 | train_loss=0.4404 acc=0.851 | val_loss=0.7292 acc=0.679\n",
      "Epoch 88 | train_loss=0.3907 acc=0.867 | val_loss=0.9009 acc=0.660\n",
      "Epoch 89 | train_loss=0.3983 acc=0.880 | val_loss=0.8819 acc=0.623\n",
      "Epoch 90 | train_loss=0.4293 acc=0.847 | val_loss=0.8283 acc=0.755\n",
      "Epoch 91 | train_loss=0.3674 acc=0.839 | val_loss=0.8594 acc=0.642\n",
      "Epoch 92 | train_loss=0.3573 acc=0.851 | val_loss=0.8055 acc=0.642\n",
      "Epoch 93 | train_loss=0.3839 acc=0.847 | val_loss=0.8189 acc=0.679\n",
      "Epoch 94 | train_loss=0.4410 acc=0.843 | val_loss=0.7615 acc=0.698\n",
      "Epoch 95 | train_loss=0.3610 acc=0.876 | val_loss=0.8765 acc=0.623\n",
      "Epoch 96 | train_loss=0.3624 acc=0.880 | val_loss=0.8061 acc=0.623\n",
      "Epoch 97 | train_loss=0.3269 acc=0.896 | val_loss=0.7893 acc=0.679\n",
      "Epoch 98 | train_loss=0.3455 acc=0.884 | val_loss=0.9471 acc=0.698\n",
      "Epoch 99 | train_loss=0.4338 acc=0.855 | val_loss=0.7233 acc=0.717\n",
      "Epoch 100 | train_loss=0.3872 acc=0.867 | val_loss=0.6952 acc=0.698\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # step the scheduler with the validation loss\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7ce74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n",
      "100%|██████████| 30.1M/30.1M [00:00<00:00, 89.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_classes = 6  # normal + 5 grades\n",
    "\n",
    "# New torchvision API\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "\n",
    "weights = EfficientNet_B1_Weights.IMAGENET1K_V1  # or EfficientNet_B3_Weights.DEFAULT\n",
    "model = efficientnet_b1(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78864323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect classifier\n",
    "print(model.classifier)\n",
    "\n",
    "# Typical structure: Dropout -> Linear(in_features, 1000)\n",
    "in_features = model.classifier[1].in_features\n",
    "\n",
    "model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # multi-class\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc3673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.2219 acc=0.602 | val_loss=0.8238 acc=0.679\n",
      "Epoch 02 | train_loss=0.5746 acc=0.815 | val_loss=0.7288 acc=0.755\n",
      "Epoch 03 | train_loss=0.3394 acc=0.920 | val_loss=0.6395 acc=0.755\n",
      "Epoch 04 | train_loss=0.2419 acc=0.940 | val_loss=0.5239 acc=0.868\n",
      "Epoch 05 | train_loss=0.1810 acc=0.960 | val_loss=0.6222 acc=0.774\n",
      "Epoch 06 | train_loss=0.1360 acc=0.964 | val_loss=0.5068 acc=0.811\n",
      "Epoch 07 | train_loss=0.1663 acc=0.948 | val_loss=0.6034 acc=0.868\n",
      "Epoch 08 | train_loss=0.1756 acc=0.964 | val_loss=0.7047 acc=0.830\n",
      "Epoch 09 | train_loss=0.1420 acc=0.948 | val_loss=0.6931 acc=0.792\n",
      "Epoch 10 | train_loss=0.1472 acc=0.948 | val_loss=0.4153 acc=0.811\n",
      "Epoch 11 | train_loss=0.1472 acc=0.948 | val_loss=0.2927 acc=0.925\n",
      "Epoch 12 | train_loss=0.0777 acc=0.984 | val_loss=0.5205 acc=0.868\n",
      "Epoch 13 | train_loss=0.0797 acc=0.976 | val_loss=0.5048 acc=0.830\n",
      "Epoch 14 | train_loss=0.0296 acc=0.996 | val_loss=0.5206 acc=0.868\n",
      "Epoch 15 | train_loss=0.0290 acc=0.996 | val_loss=0.4990 acc=0.830\n",
      "Epoch 00015: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 16 | train_loss=0.0656 acc=0.984 | val_loss=0.4949 acc=0.849\n",
      "Epoch 17 | train_loss=0.0199 acc=0.996 | val_loss=0.4972 acc=0.868\n",
      "Epoch 18 | train_loss=0.0144 acc=1.000 | val_loss=0.4403 acc=0.868\n",
      "Epoch 19 | train_loss=0.0254 acc=0.996 | val_loss=0.4756 acc=0.849\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch 20 | train_loss=0.0627 acc=0.988 | val_loss=0.5497 acc=0.830\n",
      "Epoch 21 | train_loss=0.0378 acc=0.988 | val_loss=0.5297 acc=0.849\n",
      "Epoch 22 | train_loss=0.0448 acc=0.992 | val_loss=0.5192 acc=0.830\n",
      "Epoch 23 | train_loss=0.0292 acc=0.992 | val_loss=0.4863 acc=0.830\n",
      "Epoch 00023: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch 24 | train_loss=0.0278 acc=0.992 | val_loss=0.4576 acc=0.830\n",
      "Epoch 25 | train_loss=0.0249 acc=0.992 | val_loss=0.4919 acc=0.849\n",
      "Epoch 26 | train_loss=0.0272 acc=0.996 | val_loss=0.5131 acc=0.830\n",
      "Epoch 27 | train_loss=0.0188 acc=1.000 | val_loss=0.4513 acc=0.868\n",
      "Epoch 00027: reducing learning rate of group 0 to 3.0000e-08.\n",
      "Epoch 28 | train_loss=0.0215 acc=1.000 | val_loss=0.4442 acc=0.868\n",
      "Epoch 29 | train_loss=0.0428 acc=0.984 | val_loss=0.4851 acc=0.849\n",
      "Epoch 30 | train_loss=0.0510 acc=0.988 | val_loss=0.5053 acc=0.868\n",
      "Epoch 31 | train_loss=0.0383 acc=0.988 | val_loss=0.5164 acc=0.849\n",
      "Epoch 00031: reducing learning rate of group 0 to 3.0000e-09.\n",
      "Epoch 32 | train_loss=0.0253 acc=0.992 | val_loss=0.4467 acc=0.868\n",
      "Epoch 33 | train_loss=0.0382 acc=0.996 | val_loss=0.4905 acc=0.849\n",
      "Epoch 34 | train_loss=0.0324 acc=0.996 | val_loss=0.5691 acc=0.811\n",
      "Epoch 35 | train_loss=0.0470 acc=0.988 | val_loss=0.4232 acc=0.849\n",
      "Epoch 36 | train_loss=0.0195 acc=1.000 | val_loss=0.4772 acc=0.868\n",
      "Epoch 37 | train_loss=0.0138 acc=1.000 | val_loss=0.5367 acc=0.830\n",
      "Epoch 38 | train_loss=0.0328 acc=0.996 | val_loss=0.5014 acc=0.887\n",
      "Epoch 39 | train_loss=0.0198 acc=1.000 | val_loss=0.4707 acc=0.868\n",
      "Epoch 40 | train_loss=0.0386 acc=0.988 | val_loss=0.5062 acc=0.849\n",
      "Epoch 41 | train_loss=0.0380 acc=0.992 | val_loss=0.5096 acc=0.849\n",
      "Epoch 42 | train_loss=0.0270 acc=0.996 | val_loss=0.4781 acc=0.849\n",
      "Epoch 43 | train_loss=0.0245 acc=0.996 | val_loss=0.4550 acc=0.849\n",
      "Epoch 44 | train_loss=0.0247 acc=0.996 | val_loss=0.5193 acc=0.868\n",
      "Epoch 45 | train_loss=0.0295 acc=0.992 | val_loss=0.5902 acc=0.868\n",
      "Epoch 46 | train_loss=0.0208 acc=0.996 | val_loss=0.5007 acc=0.849\n",
      "Epoch 47 | train_loss=0.0205 acc=1.000 | val_loss=0.4682 acc=0.849\n",
      "Epoch 48 | train_loss=0.0152 acc=1.000 | val_loss=0.5088 acc=0.811\n",
      "Epoch 49 | train_loss=0.0594 acc=0.980 | val_loss=0.5886 acc=0.811\n",
      "Epoch 50 | train_loss=0.0296 acc=0.992 | val_loss=0.5623 acc=0.868\n",
      "Epoch 51 | train_loss=0.0264 acc=0.996 | val_loss=0.5635 acc=0.830\n",
      "Epoch 52 | train_loss=0.0525 acc=0.992 | val_loss=0.4814 acc=0.849\n",
      "Epoch 53 | train_loss=0.0306 acc=0.996 | val_loss=0.5766 acc=0.811\n",
      "Epoch 54 | train_loss=0.0345 acc=0.992 | val_loss=0.4576 acc=0.849\n",
      "Epoch 55 | train_loss=0.0403 acc=0.996 | val_loss=0.5065 acc=0.811\n",
      "Epoch 56 | train_loss=0.0155 acc=1.000 | val_loss=0.5186 acc=0.849\n",
      "Epoch 57 | train_loss=0.0262 acc=0.996 | val_loss=0.5245 acc=0.849\n",
      "Epoch 58 | train_loss=0.0211 acc=0.996 | val_loss=0.5651 acc=0.830\n",
      "Epoch 59 | train_loss=0.0281 acc=0.996 | val_loss=0.4644 acc=0.868\n",
      "Epoch 60 | train_loss=0.0306 acc=0.988 | val_loss=0.5557 acc=0.830\n",
      "Epoch 61 | train_loss=0.0209 acc=0.996 | val_loss=0.4238 acc=0.868\n",
      "Epoch 62 | train_loss=0.0370 acc=0.992 | val_loss=0.4703 acc=0.849\n",
      "Epoch 63 | train_loss=0.0394 acc=0.984 | val_loss=0.5463 acc=0.811\n",
      "Epoch 64 | train_loss=0.0650 acc=0.976 | val_loss=0.5256 acc=0.830\n",
      "Epoch 65 | train_loss=0.0380 acc=0.992 | val_loss=0.4510 acc=0.811\n",
      "Epoch 66 | train_loss=0.0335 acc=0.996 | val_loss=0.4869 acc=0.868\n",
      "Epoch 67 | train_loss=0.0274 acc=0.996 | val_loss=0.4882 acc=0.849\n",
      "Epoch 68 | train_loss=0.0317 acc=0.992 | val_loss=0.4941 acc=0.830\n",
      "Epoch 69 | train_loss=0.0234 acc=0.992 | val_loss=0.4614 acc=0.849\n",
      "Epoch 70 | train_loss=0.0141 acc=1.000 | val_loss=0.5033 acc=0.868\n",
      "Epoch 71 | train_loss=0.0255 acc=0.996 | val_loss=0.5196 acc=0.849\n",
      "Epoch 72 | train_loss=0.0381 acc=0.984 | val_loss=0.4462 acc=0.849\n",
      "Epoch 73 | train_loss=0.0236 acc=0.992 | val_loss=0.4482 acc=0.868\n",
      "Epoch 74 | train_loss=0.0293 acc=0.996 | val_loss=0.5036 acc=0.868\n",
      "Epoch 75 | train_loss=0.0235 acc=1.000 | val_loss=0.4773 acc=0.849\n",
      "Epoch 76 | train_loss=0.0468 acc=0.988 | val_loss=0.4742 acc=0.830\n",
      "Epoch 77 | train_loss=0.0262 acc=0.996 | val_loss=0.5932 acc=0.830\n",
      "Epoch 78 | train_loss=0.0222 acc=0.996 | val_loss=0.4420 acc=0.868\n",
      "Epoch 79 | train_loss=0.0344 acc=0.992 | val_loss=0.4787 acc=0.830\n",
      "Epoch 80 | train_loss=0.0589 acc=0.984 | val_loss=0.4904 acc=0.868\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)  # class indices [0..5]\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(xb)           # [B, 6]\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "num_epochs = 80\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c9ec367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | loss=0.8434 acc=0.855\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = run_epoch(test_loader, train=False)\n",
    "print(f\"TEST | loss={test_loss:.4f} acc={test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52f92714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_classes = 6  # normal + 5 grades\n",
    "\n",
    "weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "model = models.resnet18(weights=weights)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # multi-class\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7899670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.0607 acc=0.647 | val_loss=1.0680 acc=0.585\n",
      "Epoch 02 | train_loss=0.5014 acc=0.811 | val_loss=0.6734 acc=0.717\n",
      "Epoch 03 | train_loss=0.2661 acc=0.920 | val_loss=1.0697 acc=0.623\n",
      "Epoch 04 | train_loss=0.3661 acc=0.867 | val_loss=0.8503 acc=0.717\n",
      "Epoch 05 | train_loss=0.1882 acc=0.944 | val_loss=1.1665 acc=0.717\n",
      "Epoch 06 | train_loss=0.1782 acc=0.944 | val_loss=0.8187 acc=0.717\n",
      "Epoch 00006: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 07 | train_loss=0.0827 acc=0.976 | val_loss=0.7193 acc=0.717\n",
      "Epoch 08 | train_loss=0.0461 acc=0.996 | val_loss=0.7629 acc=0.717\n",
      "Epoch 09 | train_loss=0.0369 acc=0.996 | val_loss=0.6136 acc=0.736\n",
      "Epoch 10 | train_loss=0.0408 acc=0.996 | val_loss=0.6012 acc=0.736\n",
      "Epoch 11 | train_loss=0.0302 acc=0.992 | val_loss=0.6053 acc=0.755\n",
      "Epoch 12 | train_loss=0.0205 acc=1.000 | val_loss=0.5873 acc=0.755\n",
      "Epoch 13 | train_loss=0.0178 acc=1.000 | val_loss=0.6269 acc=0.717\n",
      "Epoch 14 | train_loss=0.0644 acc=0.972 | val_loss=0.7643 acc=0.717\n",
      "Epoch 15 | train_loss=0.0392 acc=0.996 | val_loss=0.7671 acc=0.698\n",
      "Epoch 16 | train_loss=0.0293 acc=0.996 | val_loss=0.8670 acc=0.717\n",
      "Epoch 00016: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Epoch 17 | train_loss=0.0346 acc=0.996 | val_loss=0.7548 acc=0.755\n",
      "Epoch 18 | train_loss=0.0172 acc=1.000 | val_loss=0.6649 acc=0.736\n",
      "Epoch 19 | train_loss=0.0214 acc=0.996 | val_loss=0.6339 acc=0.755\n",
      "Epoch 20 | train_loss=0.0161 acc=1.000 | val_loss=0.6685 acc=0.755\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.0000e-07.\n",
      "Epoch 21 | train_loss=0.0270 acc=0.996 | val_loss=0.7682 acc=0.717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 32\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     val_loss,   val_acc   \u001b[38;5;241m=\u001b[39m run_epoch(val_loader,   train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m total_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# class indices [0..5]\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)  # class indices [0..5]\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(xb)           # [B, 6]\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "num_epochs = 80\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468471a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
