{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad19e34a",
   "metadata": {},
   "source": [
    "# Papilledema: Detection + Severity Staging (Organized Notebook)\n",
    "This notebook is a cleaned, *organized* version of the work from `Untitled41 (2).ipynb`.\n",
    "It consolidates repeated cells, keeps one source of truth for datasets/models/training, and separates **training**, **evaluation**, and **inference**.\n",
    "\n",
    "**Backbones:** EfficientNet-B4 (timm)\n",
    "\n",
    "**Tasks:**\n",
    "1) Binary detection (papilledema vs not)\n",
    "2) Ordinal staging (CORAL)\n",
    "\n",
    "> Update the paths in the Config section to match your Drive/folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003dc974",
   "metadata": {},
   "source": [
    "## 0) Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit these)\n",
    "# ----------------------------\n",
    "DATASET_ROOT = \"/content/drive/MyDrive/papilloedema\"     # folder with class subfolders OR where your images live\n",
    "SPLIT_DIR    = \"/content/drive/MyDrive/papilloedema_splits\"\n",
    "WEIGHTS_DIR  = \"/content/drive/MyDrive\"                 # where .pth are saved\n",
    "\n",
    "IMG_SIZE      = 448\n",
    "BATCH_SIZE    = 8\n",
    "NUM_WORKERS   = 2\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "DET_WEIGHTS_PATH   = f\"{WEIGHTS_DIR}/best_detection_model.pth\"\n",
    "STAGE_WEIGHTS_PATH = f\"{WEIGHTS_DIR}/best_staging_model_coral.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a6de5",
   "metadata": {},
   "source": [
    "## 1) Imports, Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0fa3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, recall_score, confusion_matrix, classification_report,\n",
    "    accuracy_score, cohen_kappa_score, mean_absolute_error\n",
    ")\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebe56e",
   "metadata": {},
   "source": [
    "## 2) Build Metadata DataFrame (path, detect, stage)\n",
    "If you already have `train.csv/val.csv/test.csv`, skip to section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this logic assumes your folder names encode class/stage.\n",
    "# If your original notebook already produced df with columns: path, detect, stage,\n",
    "# you can directly load it instead.\n",
    "\n",
    "def build_df_from_folder(root: str) -> pd.DataFrame:\n",
    "    records = []\n",
    "    root = Path(root)\n",
    "\n",
    "    for cls in root.iterdir():\n",
    "        if not cls.is_dir():\n",
    "            continue\n",
    "\n",
    "        cls_name = cls.name\n",
    "        for p in cls.glob(\"*\"):\n",
    "            if p.suffix.lower() not in [\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\"]:\n",
    "                continue\n",
    "\n",
    "            # --- TODO: adapt this mapping to your folder naming ---\n",
    "            # Example assumptions:\n",
    "            # - class folders like: \"normal\", \"pap_stage1\", \"pap_stage2\", ...\n",
    "            # - detect = 1 if papilledema else 0\n",
    "            # - stage in {0..4} or {1..5}; normalize later\n",
    "            detect = 1 if \"pap\" in cls_name.lower() else 0\n",
    "\n",
    "            # crude stage parse example; modify to your needs\n",
    "            stage = 0\n",
    "            for k in [\"1\",\"2\",\"3\",\"4\",\"5\"]:\n",
    "                if f\"stage{k}\" in cls_name.lower():\n",
    "                    stage = int(k)\n",
    "                    break\n",
    "\n",
    "            records.append({\"path\": str(p), \"detect\": detect, \"stage\": stage})\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No images found. Check DATASET_ROOT and folder structure.\")\n",
    "    return df\n",
    "\n",
    "df = build_df_from_folder(DATASET_ROOT)\n",
    "print(df.head())\n",
    "print(df.groupby([\"detect\",\"stage\"]).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84e247",
   "metadata": {},
   "source": [
    "## 3) Train/Val/Test Split (stratified by detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9cbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.30, stratify=df[\"detect\"], random_state=RANDOM_STATE\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.50, stratify=temp_df[\"detect\"], random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "train_df.to_csv(f\"{SPLIT_DIR}/train.csv\", index=False)\n",
    "val_df.to_csv(f\"{SPLIT_DIR}/val.csv\", index=False)\n",
    "test_df.to_csv(f\"{SPLIT_DIR}/test.csv\", index=False)\n",
    "\n",
    "def check_dist(d, name):\n",
    "    print(f\"\n",
    "{name} detect dist\")\n",
    "    print(d[\"detect\"].value_counts(normalize=True))\n",
    "    if (d[\"detect\"]==1).any():\n",
    "        print(f\"{name} stage dist (detect=1 only)\")\n",
    "        print(d[d[\"detect\"]==1][\"stage\"].value_counts())\n",
    "\n",
    "check_dist(train_df,\"Train\")\n",
    "check_dist(val_df,\"Val\")\n",
    "check_dist(test_df,\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff39bf",
   "metadata": {},
   "source": [
    "## 4) Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=512),\n",
    "    A.PadIfNeeded(512, 512, border_mode=0),\n",
    "    A.RandomCrop(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.4),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=512),\n",
    "    A.PadIfNeeded(512, 512, border_mode=0),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6857498",
   "metadata": {},
   "source": [
    "## 5) Dataset + Dataloaders (single implementation)\n",
    "Supports optional fundus cropping and returning the image path (useful for Grad-CAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_fundus(img_rgb, pad: int = 8):\n",
    "    \"\"\"Basic black-border removal used in the original notebook.\n",
    "    Keep/adjust based on your results.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    _, th = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts:\n",
    "        return img_rgb\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    x = max(0, x-pad); y = max(0, y-pad)\n",
    "    w = min(img_rgb.shape[1]-x, w+2*pad)\n",
    "    h = min(img_rgb.shape[0]-y, h+2*pad)\n",
    "    return img_rgb[y:y+h, x:x+w]\n",
    "\n",
    "class PapilledemaDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, use_fundus_crop=False, return_path=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.use_fundus_crop = use_fundus_crop\n",
    "        self.return_path = return_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"path\"]\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.use_fundus_crop:\n",
    "            img = crop_to_fundus(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "\n",
    "        detect = torch.tensor(row[\"detect\"], dtype=torch.float32)\n",
    "        stage  = torch.tensor(row[\"stage\"], dtype=torch.long)\n",
    "\n",
    "        if self.return_path:\n",
    "            return img, detect, stage, path\n",
    "        return img, detect, stage\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    PapilledemaDataset(train_df, train_transform, use_fundus_crop=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    PapilledemaDataset(val_df, val_transform, use_fundus_crop=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    PapilledemaDataset(test_df, val_transform, use_fundus_crop=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "x, d, s = next(iter(train_loader))\n",
    "print(\"Batch:\", x.shape, d[:5], s[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f0e71",
   "metadata": {},
   "source": [
    "## 6) Models\n",
    "### 6.1 Detection (binary)\n",
    "### 6.2 Staging (CORAL ordinal)\n",
    "\n",
    "The original notebook uses EfficientNet-B4 via `timm` and a small MLP head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionModel(nn.Module):\n",
    "    def __init__(self, backbone=\"efficientnet_b4\", pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained, num_classes=0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        logits = self.head(feat)\n",
    "        return logits.squeeze(1)\n",
    "\n",
    "# ---------- CORAL utilities ----------\n",
    "def labels_to_coral(stage_labels: torch.Tensor, num_classes: int):\n",
    "    \"\"\"Convert class labels (0..K-1) to CORAL targets of shape [B, K-1].\"\"\"\n",
    "    # For stage y, coral target is [1,1,...,1,0,0,...] length K-1\n",
    "    B = stage_labels.shape[0]\n",
    "    coral = torch.zeros((B, num_classes-1), device=stage_labels.device)\n",
    "    for k in range(num_classes-1):\n",
    "        coral[:, k] = (stage_labels > k).float()\n",
    "    return coral\n",
    "\n",
    "def coral_logits_to_label(logits: torch.Tensor):\n",
    "    \"\"\"Convert CORAL logits [B,K-1] to predicted labels 0..K-1.\"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return (probs > 0.5).sum(dim=1)\n",
    "\n",
    "class StagingModelCORAL(nn.Module):\n",
    "    def __init__(self, backbone=\"efficientnet_b4\", pretrained=True, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained, num_classes=0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes-1)  # CORAL uses K-1 outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.head(feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa500d",
   "metadata": {},
   "source": [
    "## 7) Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6100033",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_detection(model, loader, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    losses, probs_all, y_all = [], [], []\n",
    "    for images, detect, _ in loader:\n",
    "        images = images.to(device)\n",
    "        detect = detect.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, detect)\n",
    "        losses.append(loss.item())\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        probs_all.extend(probs.tolist())\n",
    "        y_all.extend(detect.detach().cpu().numpy().tolist())\n",
    "\n",
    "    probs_all = np.array(probs_all)\n",
    "    y_all = np.array(y_all).astype(int)\n",
    "    auc = roc_auc_score(y_all, probs_all)\n",
    "    recall = recall_score(y_all, probs_all >= threshold)\n",
    "    return float(np.mean(losses)), float(auc), float(recall)\n",
    "\n",
    "def train_one_epoch_detection(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    losses=[]\n",
    "    for images, detect, _ in loader:\n",
    "        images = images.to(device)\n",
    "        detect = detect.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, detect)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_stage(model, loader, criterion, num_classes: int):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    for images, detect, stage in loader:\n",
    "        # Evaluate staging only on detect==1, like your original logic\n",
    "        mask = (detect == 1)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        images = images[mask].to(device)\n",
    "        stage = stage[mask].to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        target = labels_to_coral(stage, num_classes=num_classes)\n",
    "        loss = criterion(logits, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = coral_logits_to_label(logits)\n",
    "        y_true.extend(stage.detach().cpu().numpy().tolist())\n",
    "        y_pred.extend(pred.detach().cpu().numpy().tolist())\n",
    "\n",
    "    if len(y_true)==0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return float(np.mean(losses)), float(mae), float(qwk), float(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d5c97",
   "metadata": {},
   "source": [
    "## 8) Train Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model = DetectionModel(pretrained=True).to(device)\n",
    "\n",
    "num_pos = train_df[\"detect\"].sum()\n",
    "num_neg = len(train_df) - num_pos\n",
    "pos_weight = torch.tensor([num_neg / max(num_pos,1)], device=device)\n",
    "\n",
    "det_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "det_optimizer = optim.AdamW(det_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "det_scheduler = optim.lr_scheduler.ReduceLROnPlateau(det_optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "\n",
    "EPOCHS = 20\n",
    "best_auc = -1\n",
    "for epoch in range(EPOCHS):\n",
    "    tr = train_one_epoch_detection(det_model, train_loader, det_optimizer, det_criterion)\n",
    "    vl, auc, rec = validate_detection(det_model, val_loader, det_criterion)\n",
    "    det_scheduler.step(auc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | TrainLoss {tr:.4f} | ValLoss {vl:.4f} | ValAUC {auc:.4f} | ValRecall {rec:.4f}\")\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(det_model.state_dict(), DET_WEIGHTS_PATH)\n",
    "        print(\"✅ Saved:\", DET_WEIGHTS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f72cf9",
   "metadata": {},
   "source": [
    "## 9) Train Staging (CORAL)\n",
    "You can train staging on only papilledema images, but this notebook keeps a simple approach: use the same loader and mask detect==1 inside validation.\n",
    "If you have a separate stage-only dataframe, build a dedicated loader for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STAGE_CLASSES = 4  # <-- set to your true number of stages (e.g., 4 or 5). Must match your labels.\n",
    "\n",
    "stage_model = StagingModelCORAL(pretrained=True, num_classes=NUM_STAGE_CLASSES).to(device)\n",
    "stage_criterion = nn.BCEWithLogitsLoss()\n",
    "stage_optimizer = optim.AdamW(stage_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "stage_scheduler = optim.lr_scheduler.ReduceLROnPlateau(stage_optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "\n",
    "EPOCHS_STAGE = 15\n",
    "best_qwk = -1\n",
    "for epoch in range(EPOCHS_STAGE):\n",
    "    # Train only on detect==1 samples\n",
    "    stage_model.train()\n",
    "    losses=[]\n",
    "    for images, detect, stage in train_loader:\n",
    "        mask = (detect == 1)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        images = images[mask].to(device)\n",
    "        stage = stage[mask].to(device)\n",
    "\n",
    "        stage_optimizer.zero_grad(set_to_none=True)\n",
    "        logits = stage_model(images)\n",
    "        target = labels_to_coral(stage, num_classes=NUM_STAGE_CLASSES)\n",
    "        loss = stage_criterion(logits, target)\n",
    "        loss.backward()\n",
    "        stage_optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    tr_loss = float(np.mean(losses)) if losses else np.nan\n",
    "    vl_loss, mae, qwk, acc = validate_stage(stage_model, val_loader, stage_criterion, num_classes=NUM_STAGE_CLASSES)\n",
    "    stage_scheduler.step(qwk)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{EPOCHS_STAGE} | TrainLoss {tr_loss:.4f} | ValLoss {vl_loss:.4f} | MAE {mae:.3f} | QWK {qwk:.3f} | Acc {acc:.3f}\")\n",
    "\n",
    "    if qwk > best_qwk:\n",
    "        best_qwk = qwk\n",
    "        torch.save(stage_model.state_dict(), STAGE_WEIGHTS_PATH)\n",
    "        print(\"✅ Saved:\", STAGE_WEIGHTS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c158b",
   "metadata": {},
   "source": [
    "## 10) Final Evaluation on Test Set (Detection + Staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "det_model = DetectionModel(pretrained=False).to(device)\n",
    "det_model.load_state_dict(torch.load(DET_WEIGHTS_PATH, map_location=device))\n",
    "det_model.eval()\n",
    "\n",
    "stage_model = StagingModelCORAL(pretrained=False, num_classes=NUM_STAGE_CLASSES).to(device)\n",
    "stage_model.load_state_dict(torch.load(STAGE_WEIGHTS_PATH, map_location=device))\n",
    "stage_model.eval()\n",
    "\n",
    "# ---- Detection report ----\n",
    "@torch.no_grad()\n",
    "def get_detection_probs(model, loader):\n",
    "    model.eval()\n",
    "    probs_all, y_all = [], []\n",
    "    for images, detect, _ in loader:\n",
    "        images = images.to(device)\n",
    "        logits = model(images)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        probs_all.extend(probs.tolist())\n",
    "        y_all.extend(detect.numpy().tolist())\n",
    "    return np.array(y_all).astype(int), np.array(probs_all)\n",
    "\n",
    "y_true, probs = get_detection_probs(det_model, test_loader)\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "print(\"Detection AUC:\", roc_auc_score(y_true, probs))\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# ---- Staging report (only detect==1) ----\n",
    "@torch.no_grad()\n",
    "def stage_on_test(det_model, stage_model, loader):\n",
    "    det_model.eval(); stage_model.eval()\n",
    "    y_true_s=[]; y_pred_s=[]\n",
    "    for images, detect, stage in loader:\n",
    "        images = images.to(device)\n",
    "        # If you want to gate staging by predicted detection, replace detect with predictions here.\n",
    "        mask = (detect == 1)\n",
    "        if mask.sum()==0:\n",
    "            continue\n",
    "        logits = stage_model(images[mask])\n",
    "        pred = coral_logits_to_label(logits).cpu().numpy()\n",
    "        y_true_s.extend(stage[mask].numpy().tolist())\n",
    "        y_pred_s.extend(pred.tolist())\n",
    "    return np.array(y_true_s), np.array(y_pred_s)\n",
    "\n",
    "y_s, p_s = stage_on_test(det_model, stage_model, test_loader)\n",
    "if len(y_s):\n",
    "    print(\"Staging MAE:\", mean_absolute_error(y_s, p_s))\n",
    "    print(\"Staging QWK:\", cohen_kappa_score(y_s, p_s, weights=\"quadratic\"))\n",
    "    print(\"Staging Acc:\", accuracy_score(y_s, p_s))\n",
    "    print(confusion_matrix(y_s, p_s))\n",
    "else:\n",
    "    print(\"No detect==1 samples found in test split (check labels).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97ea9f",
   "metadata": {},
   "source": [
    "## 11) Inference Helper (single image)\n",
    "This mirrors the end of your original notebook: optional fundus crop, transforms, then detection → staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one_image(image_path: str, det_threshold=0.5, use_crop=True):\n",
    "    # read\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if use_crop:\n",
    "        img = crop_to_fundus(img)\n",
    "\n",
    "    # keep a copy for display\n",
    "    vis = img.copy()\n",
    "\n",
    "    # model input\n",
    "    x = val_transform(image=img)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    # detect\n",
    "    det_logit = det_model(x)\n",
    "    det_prob = torch.sigmoid(det_logit).item()\n",
    "    det_pred = int(det_prob >= det_threshold)\n",
    "\n",
    "    # stage\n",
    "    stage_pred = None\n",
    "    if det_pred == 1:\n",
    "        stage_logits = stage_model(x)\n",
    "        stage_pred = int(coral_logits_to_label(stage_logits).item())\n",
    "\n",
    "    return vis, det_prob, det_pred, stage_pred\n",
    "\n",
    "# Example:\n",
    "# vis, prob, det, stage = predict_one_image(\"/content/drive/MyDrive/some_image.jpg\")\n",
    "# plt.imshow(vis); plt.axis(\"off\")\n",
    "# print(\"det_prob:\", prob, \"det:\", det, \"stage:\", stage)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
